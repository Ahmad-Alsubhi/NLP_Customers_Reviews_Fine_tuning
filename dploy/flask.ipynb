{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a7mad/Desktop/git_lab/ironhack/ironhack/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5001\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [14/Apr/2025 09:37:50] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:39:04] \"\u001b[35m\u001b[1mPOST / HTTP/1.1\u001b[0m\" 500 -\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a7mad/Desktop/git_lab/ironhack/ironhack/lib/python3.10/site-packages/flask/app.py\", line 1536, in __call__\n",
      "    return self.wsgi_app(environ, start_response)\n",
      "  File \"/Users/a7mad/Desktop/git_lab/ironhack/ironhack/lib/python3.10/site-packages/flask/app.py\", line 1514, in wsgi_app\n",
      "    response = self.handle_exception(e)\n",
      "  File \"/Users/a7mad/Desktop/git_lab/ironhack/ironhack/lib/python3.10/site-packages/flask/app.py\", line 1511, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/Users/a7mad/Desktop/git_lab/ironhack/ironhack/lib/python3.10/site-packages/flask/app.py\", line 919, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/Users/a7mad/Desktop/git_lab/ironhack/ironhack/lib/python3.10/site-packages/flask/app.py\", line 917, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/Users/a7mad/Desktop/git_lab/ironhack/ironhack/lib/python3.10/site-packages/flask/app.py\", line 902, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n",
      "  File \"/var/folders/51/yn8s64cn1k9g94w1rvvfrc140000gn/T/ipykernel_1900/709115252.py\", line 111, in index\n",
      "    mode = request.form['mode']\n",
      "  File \"/Users/a7mad/Desktop/git_lab/ironhack/ironhack/lib/python3.10/site-packages/werkzeug/datastructures/structures.py\", line 238, in __getitem__\n",
      "    raise exceptions.BadRequestKeyError(key)\n",
      "werkzeug.exceptions.BadRequestKeyError: 400 Bad Request: The browser (or proxy) sent a request that this server could not understand.\n",
      "KeyError: 'mode'\n",
      "127.0.0.1 - - [14/Apr/2025 09:39:04] \"\u001b[36mGET /?__debugger__=yes&cmd=resource&f=debugger.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:39:04] \"\u001b[36mGET /?__debugger__=yes&cmd=resource&f=style.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:39:04] \"GET /?__debugger__=yes&cmd=resource&f=console.png&s=kvnS1crvRNCEKyPtldQy HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:39:04] \"\u001b[36mGET /?__debugger__=yes&cmd=resource&f=console.png HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:39:26] \"GET / HTTP/1.1\" 200 -\n",
      "/Users/a7mad/Desktop/git_lab/ironhack/ironhack/lib/python3.10/site-packages/transformers/generation/utils.py:1666: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [14/Apr/2025 09:40:14] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:40:14] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:40:32] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:43:44] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:43:53] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:44:04] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:47:30] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:47:38] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:47:44] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:49:22] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:49:27] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:49:35] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:50:11] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:50:11] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:55:21] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:55:43] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:58:59] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:59:18] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Apr/2025 09:59:36] \"POST / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "import torch\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, AutoModelForSeq2SeqLM\n",
    "\n",
    "# === ÿßŸÑÿ•ÿπÿØÿßÿØ ===\n",
    "app = Flask(__name__)\n",
    "\n",
    "# === ÿ™ÿ≠ŸÖŸäŸÑ ŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑÿ™ÿµŸÜŸäŸÅ ===\n",
    "cls_tokenizer = AutoTokenizer.from_pretrained(\"../classification/transforms_fine_tuning2\")\n",
    "cls_model = AutoModelForSequenceClassification.from_pretrained(\"../classification/transforms_fine_tuning2\")\n",
    "cls_model.eval()\n",
    "\n",
    "label_map = {0: \"Negative\", 1: \"Positive\", 2: \"Neutral\"}\n",
    "\n",
    "# === ÿ™ÿ≠ŸÖŸäŸÑ ŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑÿ™ŸÑÿÆŸäÿµ ===\n",
    "kmeans_model = joblib.load(\"../claster/kmeans_model.pkl\")\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(\"../summary/my_bart_summary\")\n",
    "gen_model = AutoModelForSeq2SeqLM.from_pretrained(\"../summary/my_bart_summary\")\n",
    "gen_model.eval()\n",
    "cls_model2 = AutoModel.from_pretrained(\"../classification/transforms_fine_tuning2\")\n",
    "cls_model2.eval()\n",
    "\n",
    "cluster_names = {\n",
    "    0: \"Entry-Level and Kids Fire Tablets\",\n",
    "    1: \"Batteries, Laptop Gear, and Basic Accessories\",\n",
    "    2: \"Streaming Devices and E-Readers\",\n",
    "    3: \"Advanced E-Readers and Smart Assistants\",\n",
    "    4: \"Echo Speakers and Smart Home Hubs\"\n",
    "}\n",
    "\n",
    "# ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™\n",
    "reviews_df = pd.read_csv(\"../claster/done.csv\")\n",
    "\n",
    "# === ÿØŸàÿßŸÑ ===\n",
    "def classify_review(text):\n",
    "    inputs = cls_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=64)\n",
    "    with torch.no_grad():\n",
    "        outputs = cls_model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1).item()\n",
    "    return label_map[pred]\n",
    "\n",
    "def get_cluster(text):\n",
    "    inputs = cls_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=64)\n",
    "    with torch.no_grad():\n",
    "        output = cls_model2(**inputs)\n",
    "    token_embeddings = output.last_hidden_state\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    mask_exp = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    summed = torch.sum(token_embeddings * mask_exp, 1)\n",
    "    summed_mask = torch.clamp(mask_exp.sum(1), min=1e-9)\n",
    "    mean = (summed / summed_mask).cpu().numpy()\n",
    "    cluster_num = kmeans_model.predict(mean)[0]\n",
    "    return cluster_names[cluster_num]\n",
    "\n",
    "def summarize_review(user_text):\n",
    "    category = get_cluster(user_text)\n",
    "    df1 = reviews_df[reviews_df[\"cluster\"] == category]\n",
    "\n",
    "    top_rated = df1[df1[\"reviews.rating\"] == 5]\n",
    "    top_3 = top_rated[\"name\"].value_counts().head(3).index.tolist()\n",
    "    differences = \"\\n\".join([f\"- {i+1}. {name}\" for i, name in enumerate(top_3)])\n",
    "\n",
    "    complaints = {}\n",
    "    negative = df1[(df1[\"reviews.rating\"] <= 2) & (df1[\"reviews.doRecommend\"] == False)]\n",
    "    for prod in top_3:\n",
    "        texts = negative[negative[\"name\"] == prod][\"reviews.text\"]\n",
    "        sample = texts.sample(min(3, len(texts))) if len(texts) > 0 else []\n",
    "        complaints[prod] = \" | \".join(sample)\n",
    "\n",
    "    worst_df = df1[df1[\"reviews.doRecommend\"] == False]\n",
    "    if not worst_df.empty:\n",
    "        worst_product = worst_df[\"name\"].value_counts().idxmax()\n",
    "        worst_reasons = worst_df[worst_df[\"name\"] == worst_product][\"reviews.text\"].sample(min(3, len(worst_df))).tolist()\n",
    "    else:\n",
    "        worst_product = \"ÿ∫Ÿäÿ± ŸÖÿ™ŸàŸÅÿ±\"\n",
    "        worst_reasons = [\"ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿ£ÿ≥ÿ®ÿßÿ® Ÿàÿßÿ∂ÿ≠ÿ©.\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "üì¶ Cluster: {category}\n",
    "\n",
    "‚úÖ Top 3 Products:\n",
    "{differences}\n",
    "\n",
    "üîç Key Differences:\n",
    "Explain how these products differ in features, design, or value.\n",
    "\n",
    "‚ö†Ô∏è Top Complaints:\n",
    "- {top_3[0]}: {complaints.get(top_3[0], '')}\n",
    "- {top_3[1]}: {complaints.get(top_3[1], '')}\n",
    "- {top_3[2]}: {complaints.get(top_3[2], '')}\n",
    "\n",
    "üö´ Worst Product:\n",
    "{worst_product}\n",
    "Reasons to avoid:\n",
    "{\" | \".join(worst_reasons)}\n",
    "\"\"\"\n",
    "\n",
    "    inputs = gen_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=1024)\n",
    "    summary_ids = gen_model.generate(inputs[\"input_ids\"], max_length=300, num_beams=4, early_stopping=True)\n",
    "    output = gen_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return output\n",
    "\n",
    "# === ÿßŸÑŸÖÿ≥ÿßÿ±ÿßÿ™ ===\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def index():\n",
    "    result = \"\"\n",
    "    if request.method == 'POST':\n",
    "        mode = request.form['mode']\n",
    "        text = request.form['user_text']\n",
    "        if mode == 'classification':\n",
    "            result = classify_review(text)\n",
    "        elif mode == 'summarization':\n",
    "            result = summarize_review(text)\n",
    "    return render_template(\"index.html\", result=result)\n",
    "\n",
    "# === ÿ™ÿ¥ÿ∫ŸäŸÑ ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇ ===\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, use_reloader=False, port=5001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
